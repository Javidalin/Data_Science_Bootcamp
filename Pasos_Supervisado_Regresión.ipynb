{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprendizaje Supervisado, Problema de Regresión (numérica continua):\n",
    "\n",
    "1. Entendimiento del problema (selección de la métrica más adecuada: MAE, MSE, RMSE, MAPE, R2)\n",
    "2. Obtención de datos y primer contacto  \n",
    "3. Train y Test  <- PARTIMOS DE AQUI  \n",
    "4. MiniEDA: \n",
    "    - Análisis del target (numérica): Distribución gaussiana o no (si es gaussiana puedo usar regresor lineal)\n",
    "    - Análisis bivariante:\n",
    "        Numérica-categórica (distribución histogramas, si discrimina o no)\n",
    "        Numérica-numérica:\n",
    "            - Matriz de correlación -> Correlación contra el target (selección según criterio o no)\n",
    "                - Importante: en términos absolutos, correlación y anticorrelación son importantes\n",
    "            - De nuevo,  matriz de correlación -> Colinealidad entre numéricas sin el target (selección según criterio o no)\n",
    "    - Entendimiento de las features, selección de las mismas (si es necesario, podemos hacer primer nivel, segundo nivel...)\n",
    "5. Preparación del dataset de Train, en función del modelo (sí es sensible a la escala o no):\n",
    "    - No es sensible a la escala (árboles, ensembles):\n",
    "        - Si todo está en variables numéricas, ya no hay que hacer nada más\n",
    "        - Si hay que hacer conversión de categóricas:\n",
    "            - Binarias -> 0-1\n",
    "            - Ordinales -> Ordinal Encoder o mapeo (\"enero\":1 , \"febrero\":2 , \"marzo\":3,...)\n",
    "            - No ordinales -> OneHot Enconding (pd.get_dummies u OneHotEncoder Scikit-learn) u ordinal free style\n",
    "    - Si es sensible a la escala (regresor lineal, polinómico, etc.):\n",
    "        - Conversión de categóricas:\n",
    "            - Binarias -> 0-1\n",
    "            - Ordinales -> Ordinal Encoder o mapeo (\"enero\":1 , \"febrero\":2 , \"marzo\":3,...)\n",
    "            - No ordinales -> OneHot Enconding (pd.get_dummies u OneHotEncoder Scikit-learn) u ordinal free style\n",
    "        - Tratamiento de numéricas:\n",
    "            - Empujoncito a las que se puedan beneficiar (distribución log normal, alta concentración en valores bajos y rangos muy altos) -> Transformación con log10, o sqrt, o cbrt, etc.\n",
    "            - Después de transformar -> Escalar:\n",
    "                - Normalización MinMax [0,1] o [-1,1]\n",
    "                - Estandarización StandardScaler -> todo centrado entorno a standard dev \n",
    "    - MUY IMPORTANTE: TODO LO APLICADO A TRAIN SE APLICA A TEST !!!!!!!!\n",
    "6. Selección e instanciación de modelos. Baseline.\n",
    "7. Comparación de modelos (normalmente sin optimizar por eficiencia):\n",
    "    - Validación cruzada y en base a la métrica de negocio (MAE, MSE, RMSE, MAPE, R2)\n",
    "    - Nota: lo haremos por comparación con validación, puedes hacerlo por comparación de modelos de hiperparámetros optimizados, si así lo prefieres)  TIEMPO es la clave -> 1) Tuneo/Ajusto los hiperparámetros; 2) validación cruzada (set de validación)\n",
    "8. Selección de modelo -> Optimización de hiperparámetros (ten en cuenta la nota de 7) con GridSearchCV o RandomizedSearchCV\n",
    "9. Si el modelo es terrible por culpa de los datos:\n",
    "    - Valorar cambiar a un problema de clafisicación, dividir en varios modelos, tratar outliers, etc.\n",
    "10. Evaluación de métricas finales tras el tuneo o el equilibrado.\n",
    "10. Evaluación contra test.  \n",
    "11. Análisis de errores, posibles acciones futuras.  \n",
    "12. EXTRA: Persistencia del modelo en disco.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
